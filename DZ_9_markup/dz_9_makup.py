# -*- coding: utf-8 -*-
"""DZ_9_makup.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DbJw-wsyq818R-HMI7I4LlsX9wyfY38L
"""

import pandas as pd
from textblob import TextBlob
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import Pipeline
import nltk

nltk.download('punkt')

"""Данные были извлечены с платформы kaggle (https://www.kaggle.com/competitions/product-reviews-sentiment-analysis-light/data?select=products_sentiment_train.tsv). В этом датасете собраны отзывы на различные товары, и необходимо провести классификацию всех отзывов на 2 категории: 1 - положительные, 0 - отрицательные. В файле train.csv отзывы уже имеют метки. Мы разделим датасет на несколько частей: в первой части будут отзывы с уже имеющимися метками, во второй части мы проведем разметку с помощью библиотеки textblob, в третьей части проведем разметку в Label Studio, а в четвертой части создадим правила для разметки."""

df = pd.read_csv('products_sentiment_train.tsv', delimiter='\t')
df.head()

df.columns

"""Переименуем колонки"""

df.rename(columns={'2 . take around 10,000 640x480 pictures .': 'review', '1': 'mark'}, inplace=True)
df.head()

"""Разделим датасет на части"""

size_1 = int(len(df) * 0.75)  # 75% данных от общего размера
size_2 = int(len(df) * 0.10)  # 10% данных от общего размера
size_3 = int(len(df) * 0.05)  # 5% данныхот общего размера

part_1 = df.iloc[:size_1] # часть данных с отметкой
part_2 = df.iloc[size_1:size_1+size_2] # 10% для разметки c textblob
part_3 = df.iloc[size_1+size_2:size_1+size_2+size_2] # 10% для разметки по правилам
part_4 = df.iloc[size_1+size_2+size_2:] # 5% для разметки в Label Studio

df.shape

part_1.shape[0], part_2.shape[0], part_3.shape[0], part_4.shape[0]

"""## Проведём разметку с textblob"""

part_2.head()

def get_marked(review):
    blob = TextBlob(review)
    marked = blob.sentiment.polarity
    return marked

def get_mark_textblob(marked):
    if marked >= 0:
        return 1
    return 0

part_2['sentiment'] = part_2.review.apply(get_marked)
part_2['mark'] = part_2.sentiment.apply(get_mark_textblob)
part_2.head()

part_2.drop(columns=['sentiment'], inplace=True)
part_2.head()

part_2.hist();

"""## Разметка с помощью заданных правил

В этой части набора данных мы определим классы на основе присутствия положительных слов в отзывах. Если отзыв не содержит таких слов, то ему будет присвоена метка 0, что может означать нейтральный или отрицательный отзыв.
"""

positive_words = ['good', 'great', 'amazing', 'recommend', 'creative', 'fine', 'comfortable', 'pretty',
                 'as well', 'fantastically', 'terrific', 'love', 'like', 'easy', 'winner', 'perfect', 'nice',
                 'flawlessly', 'happy', 'fast', 'marvel', 'powerful', 'solve', 'bright', 'surprisingly',
                 'excellent', 'outstanding', 'wonderful', 'superb', 'delightful', 'success', 'impressive',
                 'brilliant', 'charming', 'genius', 'pleasure', 'vibrant', 'thrilling', 'exceptional', 'joyful',
                 'breathtaking', 'captivating', 'sweet', 'magical', 'gorgeous', 'exquisite', 'remarkable']

part_3.head()

def get_sentiment_by_rule(review):
    words = nltk.word_tokenize(review.lower())
    num_positive = sum([1 for word in words if word in positive_words])
    if num_positive > 0:
        return 1
    return 0

part_3['mark'] = part_3.review.apply(get_sentiment_by_rule)
part_3.head()

part_3.hist();

"""## Разметка данных с помощью Label Studio"""

part_4 = part_4[['review']]
part_4.head()

part_4.to_csv('part_4.csv', index=False)

from IPython.display import Image # needed to embed an image
Image('photo1.png')

Image('photo2.png')

Image('photo3.png')

part_4_new = pd.read_csv('new_part4.csv')
part_4 = part_4_new[['review', 'sentiment']]
part_4.head()

part_4.rename(columns={'sentiment': 'mark'}, inplace=True)

part_4.info()

part_4.hist();

"""Объединим все части датасета"""

data = pd.concat([part_1, part_2, part_3, part_4])

data.shape

data.reset_index(drop=True, inplace=True)

data.head()

data.hist();

"""## Проведём обучение модели"""

X_train, X_test, y_train, y_test = train_test_split(data['review'], data['mark'], test_size=0.25, random_state=101)

pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(stop_words='english')),
    ('clf', LogisticRegression(max_iter=1000))
])

pipeline.fit(X_train, y_train)

predictions = pipeline.predict(X_test)

accuracy = accuracy_score(y_test, predictions)
print(f'Точность модели равна {accuracy}')

report = classification_report(y_test, predictions)
print(report)

"""## Обучим модель на первоначальном датасете"""

X_train, X_test, y_train, y_test = train_test_split(df['review'], df['mark'], test_size=0.25, random_state=101)

pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')),
                     ('clf', LogisticRegression(max_iter=1000))])

pipeline.fit(X_train, y_train)

predictions = pipeline.predict(X_test)

accuracy = accuracy_score(y_test, predictions)
print(f'Точность модели равна {accuracy}')

report = classification_report(y_test, predictions)
print(report)

"""Эта модель немного улучшена. Разметка данных имеет большое значение для успешной работы модели машинного обучения. Правильная и информативная разметка данных играет важную роль в создании эффективной модели. Таким образом, использование заданных мной правил и библиотеки textblob для разметки данных повлияло на качество модели. Разметка в Label Studio может быть более точной, но требует больше времени для выполнения."""